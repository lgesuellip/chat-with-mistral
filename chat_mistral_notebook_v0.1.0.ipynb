{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain.retrievers.document_compressors.cohere_rerank import CohereRerank\n",
    "from functools import partial\n",
    "\n",
    "from state import State\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI API Key:\")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass(\"COHERE API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the benefits of using a mixture of experts model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "As an AI Engineer Expert at Mistral Company, specializing in developing cutting-edge AI technology for developers, \n",
    "your task is to analyze and offer comprehensive insights on user inquiries.\n",
    "Ensure that your final responses are user-friendly and formatted as posts.\n",
    "If the question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35turbo_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Using a mixture of experts model can offer several benefits, such as improved accuracy and performance in predicting outcomes. This model combines the strengths of multiple experts, allowing for more robust and well-rounded predictions. Additionally, it can help handle complex data patterns and provide more reliable results. Overall, utilizing a mixture of experts model can enhance the overall AI system's performance and provide more accurate predictions.\", response_metadata={'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | gpt35turbo_llm\n",
    "chain.invoke({\"query\": query})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [\n",
    "    Document(page_content=\"title=Au Large, summary= Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure.\", metadata = {\"url\":\"https://mistral.ai/news/mistral-large/\"}),\n",
    "    Document(page_content=\"title=Le Chat, summary=Our assistant is now in beta access, demonstrating what can be built with our technology.\", metadata = {\"url\":\"https://mistral.ai/news/le-chat-mistral/\"}),\n",
    "    Document(page_content=\"title=Mixtral of experts, summary=A high quality Sparse Mixture-of-Experts. \", metadata = {\"url\":\"https://mistral.ai/news/mixtral-of-experts/\"}),\n",
    "    Document(page_content=\"title=Mistral 7B, summary=The best 7B model to date, Apache 2.0\", metadata = {\"url\":\"https://mistral.ai/news/announcing-mistral-7b/\"}),\n",
    "    Document(page_content=\"title=Bringing open AI models to the frontier, summary=Why we're building Mistral AI.\", metadata = {\"url\":\"https://mistral.ai/news/about-mistral-ai/\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e5_embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "bge_embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "index_retriever = FAISS.from_documents(documents, e5_embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
    "web_retriever = partial(FAISS.from_documents, embedding=bge_embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
    "\n",
    "#from ragatouille import RAGPretrainedModel\n",
    "#colbert_reranker_model = RAGPretrainedModel.from_pretrained(pretrained_model_name_or_path=\"colbert-ir/colbertv2.0\", n_gpu=1)\n",
    "\n",
    "cohere_reranker_model = CohereRerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import index_retriever_agent, rag_website_agent, rerank_agent, expert_mistral_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_retriever_agent_partial = partial(index_retriever_agent, retriever=index_retriever, k=1)\n",
    "rag_website_agent_partial = partial(rag_website_agent, retriever=web_retriever, k=10)\n",
    "rerank_agent_partial = partial(rerank_agent, reranker=cohere_reranker_model, k=3)\n",
    "expert_mistral_agent_partial = partial(expert_mistral_agent, llm=gpt35turbo_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = StateGraph(State)\n",
    "flow.add_node(\"index_retriever_agent\", index_retriever_agent_partial)\n",
    "flow.add_node(\"rag_website_agent\", rag_website_agent_partial)\n",
    "flow.add_edge(\"index_retriever_agent\", \"rag_website_agent\")\n",
    "flow.add_node(\"rerank_agent\", rerank_agent_partial)\n",
    "flow.add_edge(\"rag_website_agent\", \"rerank_agent\")\n",
    "flow.add_node(\"generate\", expert_mistral_agent_partial)\n",
    "flow.add_edge(\"rerank_agent\", \"generate\")\n",
    "flow.add_edge(\"generate\", END)\n",
    "flow.set_entry_point(\"index_retriever_agent\")\n",
    "app = flow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'index_retriever_agent'\n",
      "{'documents': [Document(page_content='title=Mixtral of experts, summary=A high quality Sparse Mixture-of-Experts. ', metadata={'url': 'https://mistral.ai/news/mixtral-of-experts/'})],\n",
      " 'generation': None,\n",
      " 'query': 'What are the benefits of using a mixture of experts model?'}\n",
      "'rag_website_agent'\n",
      "{'documents': [Document(page_content='Mixtral of experts | Mistral AI | Frontier AI in your hands', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.Mixtral has the following capabilities.It gracefully handles a context of 32k tokens.It handles English, French,', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='DevelopersDevelopers\\nLa PlateformeDocsGuidesTechnologyTechnology\\nModelsDeploymentBusinessBusiness\\nUse casesCustomer storiesAbout UsAbout Us\\nMissionCareersNewsLe Chat\\nBuild NowLe Chat\\nBuild NowMixtral of expertsA high quality Sparse Mixture-of-Experts.December 11, 2023', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='Mistral AI teamMistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0.', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively.This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='outperforms Llama 2 70B, as well as GPT3.5, on most benchmarks.On the following figure, we measure the quality versus inference budget tradeoff. Mistral 7B and Mixtral 8x7B belong to a family of highly efficient models compared to Llama 2 models.The following table give detailed results on the figure above.Hallucination and biases. To identify possible flaws to be corrected by fine-tuning / preference modelling,', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='we measure the base model performance on BBQ/BOLD.Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark.', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.Mixtral is pre-trained on data extracted from the open Web – we train experts and routers simultaneously.PerformanceWe compare Mixtral to the Llama 2 family and the GPT3.5 base model. Mixtral matches or outperforms Llama 2 70B, as well as GPT3.5, on most benchmarks.On the following figure, we measure', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='following. On MT-Bench, it reaches a score of 8.30, making it the best open-source model, with a performance comparable to GPT3.5.Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified here. A proper preference tuning can also serve this purpose. Bear in mind that without such a prompt, the model will just follow whatever instructions are given.Deploy Mixtral with an open-source deployment stackTo', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='following capabilities.It gracefully handles a context of 32k tokens.It handles English, French, Italian, German and Spanish.It shows strong performance in code generation.It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.Pushing the frontier of open models with sparse architecturesMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'})],\n",
      " 'generation': None,\n",
      " 'query': 'What are the benefits of using a mixture of experts model?'}\n",
      "'rerank_agent'\n",
      "{'documents': [Document(page_content='following capabilities.It gracefully handles a context of 32k tokens.It handles English, French, Italian, German and Spanish.It shows strong performance in code generation.It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.Pushing the frontier of open models with sparse architecturesMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.Mixtral has the following capabilities.It gracefully handles a context of 32k tokens.It handles English, French,', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='Mistral AI teamMistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0.', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'})],\n",
      " 'generation': None,\n",
      " 'query': 'What are the benefits of using a mixture of experts model?'}\n",
      "'generate'\n",
      "{'documents': [Document(page_content='following capabilities.It gracefully handles a context of 32k tokens.It handles English, French, Italian, German and Spanish.It shows strong performance in code generation.It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.Pushing the frontier of open models with sparse architecturesMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.Mixtral has the following capabilities.It gracefully handles a context of 32k tokens.It handles English, French,', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='Mistral AI teamMistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0.', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'})],\n",
      " 'generation': '# Post\\n'\n",
      "               'The benefits of using a mixture of experts model, such as '\n",
      "               'Mixtral, include:\\n'\n",
      "               '1. **Strong Performance**: Mixtral shows strong performance in '\n",
      "               'code generation and achieves a score of 8.3 on MT-Bench.\\n'\n",
      "               '2. **Sparse Architecture**: Mixtral is a sparse '\n",
      "               'mixture-of-experts network, which offers high-quality '\n",
      "               'performance with open weights.\\n'\n",
      "               '3. **Faster Inference**: Mixtral outperforms other models on '\n",
      "               'most benchmarks with 6x faster inference, making it efficient '\n",
      "               'for real-time applications.\\n'\n",
      "               '4. **Cost-Effective**: Mixtral is the best model overall '\n",
      "               'regarding cost/performance trade-offs, matching or '\n",
      "               'outperforming other models like GPT3.5.\\n'\n",
      "               \"5. **Community Benefit**: Mistral AI team's mission is to \"\n",
      "               'deliver the best open models to the developer community, '\n",
      "               'fostering new inventions and usages with original models like '\n",
      "               'Mixtral.\\n'\n",
      "               '\\n'\n",
      "               'In conclusion, using a mixture of experts model like Mixtral '\n",
      "               'can provide high performance, efficiency, cost-effectiveness, '\n",
      "               'and community benefits for developers looking to enhance their '\n",
      "               'AI technologies.',\n",
      " 'query': 'What are the benefits of using a mixture of experts model?'}\n",
      "'__end__'\n",
      "{'documents': [Document(page_content='following capabilities.It gracefully handles a context of 32k tokens.It handles English, French, Italian, German and Spanish.It shows strong performance in code generation.It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.Pushing the frontier of open models with sparse architecturesMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.Mixtral has the following capabilities.It gracefully handles a context of 32k tokens.It handles English, French,', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='Mistral AI teamMistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0.', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'})],\n",
      " 'generation': '# Post\\n'\n",
      "               'The benefits of using a mixture of experts model, such as '\n",
      "               'Mixtral, include:\\n'\n",
      "               '1. **Strong Performance**: Mixtral shows strong performance in '\n",
      "               'code generation and achieves a score of 8.3 on MT-Bench.\\n'\n",
      "               '2. **Sparse Architecture**: Mixtral is a sparse '\n",
      "               'mixture-of-experts network, which offers high-quality '\n",
      "               'performance with open weights.\\n'\n",
      "               '3. **Faster Inference**: Mixtral outperforms other models on '\n",
      "               'most benchmarks with 6x faster inference, making it efficient '\n",
      "               'for real-time applications.\\n'\n",
      "               '4. **Cost-Effective**: Mixtral is the best model overall '\n",
      "               'regarding cost/performance trade-offs, matching or '\n",
      "               'outperforming other models like GPT3.5.\\n'\n",
      "               \"5. **Community Benefit**: Mistral AI team's mission is to \"\n",
      "               'deliver the best open models to the developer community, '\n",
      "               'fostering new inventions and usages with original models like '\n",
      "               'Mixtral.\\n'\n",
      "               '\\n'\n",
      "               'In conclusion, using a mixture of experts model like Mixtral '\n",
      "               'can provide high performance, efficiency, cost-effectiveness, '\n",
      "               'and community benefits for developers looking to enhance their '\n",
      "               'AI technologies.',\n",
      " 'query': 'What are the benefits of using a mixture of experts model?'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Iterate over the stream of outputs\n",
    "for output in app.stream(State(query=query)):\n",
    "    # Each output is a dictionary where keys are node names and values are outputs\n",
    "    for node, state_output in output.items():\n",
    "        # Print the node name\n",
    "        pprint(node)\n",
    "        pprint(state_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'documents': [Document(page_content='following capabilities.It gracefully handles a context of 32k tokens.It handles English, French, Italian, German and Spanish.It shows strong performance in code generation.It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.Pushing the frontier of open models with sparse architecturesMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.Mixtral has the following capabilities.It gracefully handles a context of 32k tokens.It handles English, French,', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'}),\n",
      "               Document(page_content='Mistral AI teamMistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages.Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0.', metadata={'source': 'https://mistral.ai/news/mixtral-of-experts/', 'title': 'Mixtral of experts | Mistral AI | Frontier AI in your hands', 'description': 'A high quality Sparse Mixture-of-Experts.', 'language': 'en-us'})],\n",
      " 'generation': '# Post\\n'\n",
      "               'The benefits of using a mixture of experts model, such as '\n",
      "               'Mixtral, include:\\n'\n",
      "               '1. **Strong Performance**: Mixtral shows strong performance in '\n",
      "               'code generation and achieves a score of 8.3 on MT-Bench.\\n'\n",
      "               '2. **Sparse Architecture**: Mixtral is a sparse '\n",
      "               'mixture-of-experts network, which offers high-quality '\n",
      "               'performance with open weights.\\n'\n",
      "               '3. **Faster Inference**: Mixtral outperforms other models on '\n",
      "               'most benchmarks with 6x faster inference, making it efficient '\n",
      "               'for real-time applications.\\n'\n",
      "               '4. **Cost-Effective**: Mixtral is the best model overall '\n",
      "               'regarding cost/performance trade-offs, matching or '\n",
      "               'outperforming other models like GPT3.5.\\n'\n",
      "               \"5. **Community Benefit**: Mistral AI team's mission is to \"\n",
      "               'deliver the best open models to the developer community, '\n",
      "               'fostering new inventions and usages with original models like '\n",
      "               'Mixtral.\\n'\n",
      "               '\\n'\n",
      "               'In conclusion, using a mixture of experts model like Mixtral '\n",
      "               'can provide high performance, efficiency, cost-effectiveness, '\n",
      "               'and community benefits for developers looking to enhance their '\n",
      "               'AI technologies.',\n",
      " 'query': 'What are the benefits of using a mixture of experts model?'}\n"
     ]
    }
   ],
   "source": [
    "output = output[\"__end__\"]\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"GOOGLE API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/laupinto/Desktop/LLMs projects/env/lib/python3.9/site-packages/deepeval/__init__.py:41: UserWarning: You are using deepeval version 0.20.98, however version 0.20.99 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from eval import GoogleGenerativeAIModel\n",
    "\n",
    "#output = output[\"__end__\"]\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=GoogleGenerativeAIModel())\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=output[\"query\"],\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=output[\"generation\"],\n",
    "    retrieval_context=[doc.page_content for doc in output[\"documents\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e3c066efcb41eeac1a1039f8720611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because there are no irrelevant statements in the actual output.\n"
     ]
    }
   ],
   "source": [
    "answer_relevancy_metric.measure(test_case)\n",
    "print(answer_relevancy_metric.score)\n",
    "# Most metrics also offer an explanation\n",
    "print(answer_relevancy_metric.reason)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
